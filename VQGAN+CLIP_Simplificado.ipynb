{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VQGAN+CLIP Simplificado\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Licensed under the MIT License\n",
    "\n",
    "# Copyright (c) 2021 Katherine Crowson\n",
    "\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Volumenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extra import * \n",
    "\n",
    "MODELS_FOLDER = Path(\"/tf/models\")\n",
    "OUTPUTS_FOLDER= Path(\"/tf/outputs\")\n",
    "SRC_FOLDER= Path(\"/tf/outputs/src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxbEW-P-tOqm"
   },
   "source": [
    "## Descargar modelos pre-entrenados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l2JAjdoUtD9Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models: 6\n",
      "['vqgan_imagenet_f16_1024', 'vqgan_imagenet_f16_16384', 'coco', 'faceshq', 'wikiart', 'sflckr']\n"
     ]
    }
   ],
   "source": [
    "from get_models import list_models, download_models\n",
    "models = list_models(return_list=True)\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading 4 models.\n",
      "\t0 - downloading 'coco' ... "
     ]
    }
   ],
   "source": [
    "download_models(models, output=MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vqgan_imagenet_f16_16384 ya existe.\n"
     ]
    }
   ],
   "source": [
    "# Selecciona un modelo\n",
    "model = models[1] \n",
    "\n",
    "if (Path(MODELS_FOLDER)/ f\"{model}.yaml\").exists():\n",
    "    print(f\"{model} ya existe.\")\n",
    "else:\n",
    "    print(f\"Descargando: {model}\")\n",
    "    download_models(model, output=MODELS_FOLDER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBdP78_stc76"
   },
   "source": [
    "**Definir parámetros principales:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls $MODELS_FOLDER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6KcZ4cBydlb6"
   },
   "outputs": [],
   "source": [
    "#Texto usado como Input para generar la imagen:\n",
    "txt = 'Todo va a salir bien'\n",
    "prompts = [txt]\n",
    "\n",
    "# Tamaño de imagen\n",
    "#size = (200, 200) #6129MiB de V-RAM\n",
    "size = (340, 340) #8693MiB de V-RAM\n",
    "#size = (450, 450) #10201MiB de V-RAM\n",
    "\n",
    "# Numero de iteraciones en la creación de la imagen, a mayor el número de iteraciones mejor es el detalle en la imagen\n",
    "iterations = 250\n",
    "\n",
    "# Modelo a usar\n",
    "# modelos descargados:\n",
    "# !ls $MODELS_FOLDER \n",
    "model = 'vqgan_imagenet_f16_1024'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definir parámetros secundarios:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"init_image\": None,\n",
    "    \"seed\": None,\n",
    "    \"step_size\": 0.2,\n",
    "    \"cutn\": 64,\n",
    "    \"cut_pow\": 1.,\n",
    "    \"display_freq\": 5,\n",
    "    \"image_prompts\": [],\n",
    "    \"noise_prompt_seeds\": [],\n",
    "    \"noise_prompt_weights\": [],\n",
    "    \"init_weight\": 0.,\n",
    "    \"clip_model\": 'ViT-B/32',\n",
    "}\n",
    "\n",
    "# SOBREESCRIBIR RESULTADOS\n",
    "overwrite=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRYC1h8hz0QO"
   },
   "source": [
    "## Generar frames del proceso resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Working with z of shape (1, 256, 16, 16) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from taming/modules/autoencoder/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "Restored from /tf/models/vqgan_imagenet_f16_1024.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:258: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  \"Argument interpolation should be of type InterpolationMode instead of int. \"\n",
      "Training: 100%|██████████| 280/280 [01:56<00:00,  2.40it/s]\n"
     ]
    }
   ],
   "source": [
    "from generate_images import *\n",
    "\n",
    "last_frame_index = generate_images(\n",
    "    prompts=prompts,\n",
    "    model=model, \n",
    "    outputs_folder=OUTPUTS_FOLDER,\n",
    "    models_folder=MODELS_FOLDER,\n",
    "    iterations=280,\n",
    "    **config,\n",
    "    overwrite=overwrite\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando frames: 100%|██████████| 278/278 [00:00<00:00, 11072.65it/s]\n",
      "Asignando frames: 100%|██████████| 278/278 [00:05<00:00, 49.94it/s]\n"
     ]
    }
   ],
   "source": [
    "experiment_name = to_experiment_name(prompts)\n",
    "experiment_folder = Path(OUTPUTS_FOLDER) / experiment_name\n",
    "create_video(last_frame_index, Path(experiment_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"temp.mp4\" controls  >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!cp $experiment_folder/video.mp4 temp.mp4\n",
    "from IPython.display import Video\n",
    "video_file = \"temp.mp4\"\n",
    "Video(video_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "Z3H1gZjHDdXI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Jun  1 02:53:44 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 460.73.01    Driver Version: 460.73.01    CUDA Version: 11.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 3060    Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 38%   52C    P2    41W / 170W |  10542MiB / 12045MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar memoria de la GPU\n",
    "reset_kernel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRTJuNI1PHVV"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Notebook Original: https://colab.research.google.com/drive/1L8oL-vLJXVcRzCFbPwOoMkPKJ8-aYdPN"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "VQGAN+CLIP_Simplificado.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
